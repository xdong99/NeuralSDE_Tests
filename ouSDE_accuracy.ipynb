{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy of the generators with different SDE solvers and BP methods.\n",
    "Time ranges: 0-64, 0-128\n",
    "\n",
    "1. Normal Backpropogation \n",
    "2. Midpoint solver with adjoint method\n",
    "3. Reversible solver with adjoint method\n",
    "\n",
    "To check for training performance, I will need to keep the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim.swa_utils as swa_utils\n",
    "import torchcde\n",
    "import torchsde\n",
    "import tqdm\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some globally definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ts = 128\n",
    "data_size = 8192\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(t_size, dataset_size, batch_size, device):\n",
    "    class OrnsteinUhlenbeckSDE(torch.nn.Module):\n",
    "        sde_type = 'ito'\n",
    "        noise_type = 'scalar'\n",
    "\n",
    "        def __init__(self, mu, theta, sigma):\n",
    "            super().__init__()\n",
    "            self.register_buffer('mu', torch.as_tensor(mu))\n",
    "            self.register_buffer('theta', torch.as_tensor(theta))\n",
    "            self.register_buffer('sigma', torch.as_tensor(sigma))\n",
    "\n",
    "        def f(self, t, y):\n",
    "            return self.mu * t - self.theta * y\n",
    "\n",
    "        def g(self, t, y):\n",
    "            return self.sigma.expand(y.size(0), 1, 1) * (2 * t / t_size)\n",
    "\n",
    "    ou_sde = OrnsteinUhlenbeckSDE(mu=0.02, theta=0.1, sigma=0.4).to(device)\n",
    "    y0 = torch.rand(dataset_size, device=device).unsqueeze(-1) * 2 - 1\n",
    "    ts = torch.linspace(0, t_size - 1, 2 * t_size, device=device)\n",
    "    ys = torchsde.sdeint(ou_sde, y0, ts, dt=1e-1)\n",
    "\n",
    "    ###################\n",
    "    # To demonstrate how to handle irregular data, then here we additionally drop some of the data (by setting it to\n",
    "    # NaN.)\n",
    "    ###################\n",
    "    ys_num = ys.numel()\n",
    "    to_drop = torch.randperm(ys_num)[:int(0.3 * ys_num)]\n",
    "    ys.view(-1)[to_drop] = float('nan')\n",
    "\n",
    "    ###################\n",
    "    # Typically important to normalise data. Note that the data is normalised with respect to the statistics of the\n",
    "    # initial data, _not_ the whole time series. This seems to help the learning process, presumably because if the\n",
    "    # initial condition is wrong then it's pretty hard to learn the rest of the SDE correctly.\n",
    "    ###################\n",
    "    y0_flat = ys[0].view(-1)\n",
    "    y0_not_nan = y0_flat.masked_select(~torch.isnan(y0_flat))\n",
    "    ys = (ys - y0_not_nan.mean()) / y0_not_nan.std()\n",
    "\n",
    "    ###################\n",
    "    # As discussed, time must be included as a channel for the discriminator.\n",
    "    ###################\n",
    "    ys = torch.cat([ts.unsqueeze(0).unsqueeze(-1).expand(dataset_size, t_size * 2, 1),\n",
    "                    ys.transpose(0, 1)], dim=2)\n",
    "    # shape (dataset_size=1000, t_size=100, 1 + data_size=3)\n",
    "\n",
    "    ###################\n",
    "    # Package up.\n",
    "    ###################\n",
    "    data_size = ys.size(-1) - 1  # How many channels the data has (not including time, hence the minus one).\n",
    "    ys_coeffs = torchcde.linear_interpolation_coeffs(ys)  # as per neural CDEs.\n",
    "    dataset = torch.utils.data.TensorDataset(ys_coeffs)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return ts, data_size, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts, data_size, train_dataloader = get_data(ts, data_size, batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving training data as I will check on it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = next(iter(train_dataloader))\n",
    "# save training_data to local\n",
    "torch.save(training_data, 'training_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other definitions to be used to monitor the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import psutil\n",
    " \n",
    "# inner psutil function\n",
    "def process_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss\n",
    " \n",
    "# decorator function\n",
    "def profile(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    " \n",
    "        mem_before = process_memory()\n",
    "        result = func(*args, **kwargs)\n",
    "        mem_after = process_memory()\n",
    "        print(\"{}:consumed memory: {:,}\".format(\n",
    "            func.__name__,\n",
    "            mem_before, mem_after, mem_after - mem_before))\n",
    " \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes related to the model structure.\n",
    "Definitions of MLP, GeneratorFunc, DiscriminatorFunc, Discriminator.\n",
    "Generator will be written later in each cases as the solvers used will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_size, out_size, mlp_size, num_layers, tanh):\n",
    "        super().__init__()\n",
    "\n",
    "        model = [torch.nn.Linear(in_size, mlp_size),\n",
    "                 LipSwish()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            model.append(torch.nn.Linear(mlp_size, mlp_size))\n",
    "        model.append(torch.nn.Linear(mlp_size, out_size))\n",
    "        if tanh:\n",
    "            model.append(torch.nn.Tanh())\n",
    "        self._model = torch.nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "class GeneratorFunc(torch.nn.Module):\n",
    "    sde_type = 'stratonovich'\n",
    "    noise_type = 'general'\n",
    "\n",
    "    def __init__(self, noise_size, hidden_size, mlp_size, num_layers):\n",
    "        super().__init__()\n",
    "        self._noise_size = noise_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._drift = MLP(1 + hidden_size, hidden_size, mlp_size, num_layers, tanh=True)\n",
    "        self._diffusion = MLP(1 + hidden_size, hidden_size * noise_size, mlp_size, num_layers, tanh=True)\n",
    "\n",
    "    def f_and_g(self, t, x):\n",
    "        # t has shape ()\n",
    "        # x has shape (batch_size, hidden_size)\n",
    "        t = t.expand(x.size(0), 1)\n",
    "        tx = torch.cat([t, x], dim=1)\n",
    "        return self._drift(tx), self._diffusion(tx).view(x.size(0), self._hidden_size, self._noise_size)\n",
    "\n",
    "class DiscriminatorFunc(torch.nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, mlp_size, num_layers):\n",
    "        super().__init__()\n",
    "        self._data_size = data_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "        # tanh is important for model performance\n",
    "        self._module = MLP(1 + hidden_size, hidden_size * (1 + data_size), mlp_size, num_layers, tanh=True)\n",
    "\n",
    "    def forward(self, t, h):\n",
    "        # t has shape ()\n",
    "        # h has shape (batch_size, hidden_size)\n",
    "        t = t.expand(h.size(0), 1)\n",
    "        th = torch.cat([t, h], dim=1)\n",
    "        return self._module(th).view(h.size(0), self._hidden_size, 1 + self._data_size)\n",
    "\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, mlp_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self._initial = MLP(1 + data_size, hidden_size, mlp_size, num_layers, tanh=False)\n",
    "        self._func = DiscriminatorFunc(data_size, hidden_size, mlp_size, num_layers)\n",
    "        self._readout = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, ys_coeffs):\n",
    "        # ys_coeffs has shape (batch_size, t_size, 1 + data_size)\n",
    "        # The +1 corresponds to time. When solving CDEs, It turns out to be most natural to treat time as just another\n",
    "        # channel: in particular this makes handling irregular data quite easy, when the times may be different between\n",
    "        # different samples in the batch.\n",
    "\n",
    "        Y = torchcde.LinearInterpolation(ys_coeffs)\n",
    "        Y0 = Y.evaluate(Y.interval[0])\n",
    "        h0 = self._initial(Y0)\n",
    "        hs = torchcde.cdeint(Y, self._func, h0, Y.interval, method='reversible_heun', backend='torchsde', dt=1.0,\n",
    "                             adjoint_method='adjoint_reversible_heun',\n",
    "                             adjoint_params=(ys_coeffs,) + tuple(self._func.parameters()))\n",
    "        score = self._readout(hs[:, -1])\n",
    "        return score.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(ts, batch_size, dataloader, generator, discriminator):\n",
    "    with torch.no_grad():\n",
    "        total_samples = 0\n",
    "        total_loss = 0\n",
    "        for real_samples, in dataloader:\n",
    "            generated_samples = generator(ts, batch_size)\n",
    "            generated_score = discriminator(generated_samples)\n",
    "            real_score = discriminator(real_samples)\n",
    "            loss = generated_score - real_score\n",
    "            total_samples += batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_normalBP(torch.nn.Module):\n",
    "    def __init__(self, data_size, initial_noise_size, noise_size, hidden_size, mlp_size, num_layers):\n",
    "        super().__init__()\n",
    "        self._initial_noise_size = initial_noise_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "        self._initial = MLP(initial_noise_size, hidden_size, mlp_size, num_layers, tanh=False)\n",
    "        self._func = GeneratorFunc(noise_size, hidden_size, mlp_size, num_layers)\n",
    "        self._readout = torch.nn.Linear(hidden_size, data_size)\n",
    "\n",
    "    def forward(self, ts, batch_size):\n",
    "        # ts has shape (t_size,) and corresponds to the points we want to evaluate the SDE at.\n",
    "\n",
    "        ###################\n",
    "        # Actually solve the SDE.\n",
    "        ###################\n",
    "        init_noise = torch.randn(batch_size, self._initial_noise_size, device=ts.device)\n",
    "        x0 = self._initial(init_noise)\n",
    "\n",
    "        ###################\n",
    "        # We use the normal BP to get accurate gradients whilst using the adjoint method.\n",
    "        ###################\n",
    "        xs = torchsde.sdeint(self._func, x0, ts, method='midpoint', dt=1.0)\n",
    "        xs = xs.transpose(0, 1)\n",
    "        ys = self._readout(xs)\n",
    "\n",
    "        ###################\n",
    "        # Normalise the data to the form that the discriminator expects, in particular including time as a channel.\n",
    "        ###################\n",
    "        ts = ts.unsqueeze(0).unsqueeze(-1).expand(batch_size, ts.size(0), 1)\n",
    "        return torchcde.linear_interpolation_coeffs(torch.cat([ts, ys], dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "\n",
    "def trainingBP(\n",
    "        # Architectural hyperparameters. These are quite small for illustrative purposes.\n",
    "        ts,             # How many timesteps to use for the SDE solver.\n",
    "        data_size,        # How many data points to use in the dataset.\n",
    "        train_dataloader,\n",
    "        initial_noise_size=5,  # How many noise dimensions to sample at the start of the SDE.\n",
    "        noise_size=3,          # How many dimensions the Brownian motion has.\n",
    "        hidden_size=16,        # How big the hidden size of the generator SDE and the discriminator CDE are.\n",
    "        mlp_size=16,           # How big the layers in the various MLPs are.\n",
    "        num_layers=1,          # How many hidden layers to have in the various MLPs.\n",
    "\n",
    "        # Training hyperparameters. Be prepared to tune these very carefully, as with any GAN.\n",
    "        generator_lr=2e-4,      # Learning rate often needs careful tuning to the problem.\n",
    "        discriminator_lr=1e-3,  # Learning rate often needs careful tuning to the problem.\n",
    "        batch_size=1024,        # Batch size.\n",
    "        steps=10000,            # How many steps to train both generator and discriminator for.\n",
    "        init_mult1=3,           # Changing the initial parameter size can help.\n",
    "        init_mult2=0.5,         #\n",
    "        weight_decay=0.01,      # Weight decay.\n",
    "        swa_step_start=5000,    # When to start using stochastic weight averaging.\n",
    "\n",
    "        # Evaluation and plotting hyperparameters\n",
    "        steps_per_print=100,                   # How often to print the loss.\n",
    "        device = 'cuda',                       # Which device to use.\n",
    "):\n",
    "    infinite_train_dataloader = (elem for it in iter(lambda: train_dataloader, None) for elem in it)\n",
    "    \n",
    "    # Models\n",
    "    generator = Generator_normalBP(data_size, initial_noise_size, noise_size, hidden_size, mlp_size, num_layers).to(device)\n",
    "    discriminator = Discriminator(data_size, hidden_size, mlp_size, num_layers).to(device)\n",
    "    # Weight averaging really helps with GAN training.\n",
    "    averaged_generator = swa_utils.AveragedModel(generator)\n",
    "    averaged_discriminator = swa_utils.AveragedModel(discriminator)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param in generator._initial.parameters():\n",
    "            param *= init_mult1\n",
    "        for param in generator._func.parameters():\n",
    "            param *= init_mult2\n",
    "\n",
    "    # Optimisers. Adadelta turns out to be a much better choice than SGD or Adam, interestingly.\n",
    "    generator_optimiser = torch.optim.Adadelta(generator.parameters(), lr=generator_lr, weight_decay=weight_decay)\n",
    "    discriminator_optimiser = torch.optim.Adadelta(discriminator.parameters(), lr=discriminator_lr,\n",
    "                                                   weight_decay=weight_decay)\n",
    "    \n",
    "    # Train both generator and discriminator.\n",
    "    t0 = time.time()\n",
    "\n",
    "    trange = tqdm.tqdm(range(steps))\n",
    "    for step in trange:\n",
    "        real_samples, = next(infinite_train_dataloader)\n",
    "\n",
    "        generated_samples = generator(ts, batch_size)\n",
    "        generated_score = discriminator(generated_samples)\n",
    "        real_score = discriminator(real_samples)\n",
    "        loss = generated_score - real_score\n",
    "        loss.backward()\n",
    "\n",
    "        for param in generator.parameters():\n",
    "            param.grad *= -1\n",
    "        generator_optimiser.step()\n",
    "        discriminator_optimiser.step()\n",
    "        generator_optimiser.zero_grad()\n",
    "        discriminator_optimiser.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for module in discriminator.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    lim = 1 / module.out_features\n",
    "                    module.weight.clamp_(-lim, lim)\n",
    "\n",
    "        # Stochastic weight averaging typically improves performance.\n",
    "        if step > swa_step_start:\n",
    "            averaged_generator.update_parameters(generator)\n",
    "            averaged_discriminator.update_parameters(discriminator)\n",
    "\n",
    "        if (step % steps_per_print) == 0 or step == steps - 1:\n",
    "            total_unaveraged_loss = evaluate_loss(ts, batch_size, train_dataloader, generator, discriminator)\n",
    "            if step > swa_step_start:\n",
    "                total_averaged_loss = evaluate_loss(ts, batch_size, train_dataloader, averaged_generator.module,\n",
    "                                                    averaged_discriminator.module)\n",
    "                trange.write(f\"Step: {step:3} Loss (unaveraged): {total_unaveraged_loss:.4f} \"\n",
    "                             f\"Loss (averaged): {total_averaged_loss:.4f}\")\n",
    "            else:\n",
    "                trange.write(f\"Step: {step:3} Loss (unaveraged): {total_unaveraged_loss:.4f}\")\n",
    "\n",
    "    print(\"Training time:\", time.time()-t0)\n",
    "    generator.load_state_dict(averaged_generator.module.state_dict())\n",
    "    discriminator.load_state_dict(averaged_discriminator.module.state_dict())\n",
    "    return generator, discriminator\n",
    "\n",
    "generator_bp, discriminator_bp = trainingBP(ts, data_size, train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralSDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
