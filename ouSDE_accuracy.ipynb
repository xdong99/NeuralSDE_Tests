{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy of the generators with different SDE solvers and BP methods.\n",
    "Time ranges: 0-64, 0-128\n",
    "\n",
    "1. Normal Backpropogation \n",
    "2. Midpoint solver with adjoint method\n",
    "3. Reversible solver with adjoint method\n",
    "\n",
    "To check for training performance, I will need to keep the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim.swa_utils as swa_utils\n",
    "import torchcde\n",
    "import torchsde\n",
    "import tqdm\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some globally definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ts = 128\n",
    "data_size = 8192\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(t_size, dataset_size, batch_size, device):\n",
    "    class OrnsteinUhlenbeckSDE(torch.nn.Module):\n",
    "        sde_type = 'ito'\n",
    "        noise_type = 'scalar'\n",
    "\n",
    "        def __init__(self, mu, theta, sigma):\n",
    "            super().__init__()\n",
    "            self.register_buffer('mu', torch.as_tensor(mu))\n",
    "            self.register_buffer('theta', torch.as_tensor(theta))\n",
    "            self.register_buffer('sigma', torch.as_tensor(sigma))\n",
    "\n",
    "        def f(self, t, y):\n",
    "            return self.mu * t - self.theta * y\n",
    "\n",
    "        def g(self, t, y):\n",
    "            return self.sigma.expand(y.size(0), 1, 1) * (2 * t / t_size)\n",
    "\n",
    "    ou_sde = OrnsteinUhlenbeckSDE(mu=0.02, theta=0.1, sigma=0.4).to(device)\n",
    "    y0 = torch.rand(dataset_size, device=device).unsqueeze(-1) * 2 - 1\n",
    "    ts = torch.linspace(0, t_size - 1, 2 * t_size, device=device)\n",
    "    ys = torchsde.sdeint(ou_sde, y0, ts, dt=1e-1)\n",
    "\n",
    "    ###################\n",
    "    # To demonstrate how to handle irregular data, then here we additionally drop some of the data (by setting it to\n",
    "    # NaN.)\n",
    "    ###################\n",
    "    ys_num = ys.numel()\n",
    "    to_drop = torch.randperm(ys_num)[:int(0.3 * ys_num)]\n",
    "    ys.view(-1)[to_drop] = float('nan')\n",
    "\n",
    "    ###################\n",
    "    # Typically important to normalise data. Note that the data is normalised with respect to the statistics of the\n",
    "    # initial data, _not_ the whole time series. This seems to help the learning process, presumably because if the\n",
    "    # initial condition is wrong then it's pretty hard to learn the rest of the SDE correctly.\n",
    "    ###################\n",
    "    y0_flat = ys[0].view(-1)\n",
    "    y0_not_nan = y0_flat.masked_select(~torch.isnan(y0_flat))\n",
    "    ys = (ys - y0_not_nan.mean()) / y0_not_nan.std()\n",
    "\n",
    "    ###################\n",
    "    # As discussed, time must be included as a channel for the discriminator.\n",
    "    ###################\n",
    "    ys = torch.cat([ts.unsqueeze(0).unsqueeze(-1).expand(dataset_size, t_size * 2, 1),\n",
    "                    ys.transpose(0, 1)], dim=2)\n",
    "    # shape (dataset_size=1000, t_size=100, 1 + data_size=3)\n",
    "\n",
    "    ###################\n",
    "    # Package up.\n",
    "    ###################\n",
    "    data_size = ys.size(-1) - 1  # How many channels the data has (not including time, hence the minus one).\n",
    "    ys_coeffs = torchcde.linear_interpolation_coeffs(ys)  # as per neural CDEs.\n",
    "    dataset = torch.utils.data.TensorDataset(ys_coeffs)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return ts, data_size, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts, data_size, train_dataloader = get_data(ts, data_size, batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving training data as I will check on it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = next(iter(train_dataloader))\n",
    "# save training_data to local\n",
    "torch.save(training_data, 'training_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Time I should Just Load from what i saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other definitions to be used to monitor the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import psutil\n",
    " \n",
    "# inner psutil function\n",
    "def process_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss\n",
    " \n",
    "# decorator function\n",
    "def profile(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    " \n",
    "        mem_before = process_memory()\n",
    "        result = func(*args, **kwargs)\n",
    "        mem_after = process_memory()\n",
    "        print(\"{}:consumed memory: {:,}\".format(\n",
    "            func.__name__,\n",
    "            mem_before, mem_after, mem_after - mem_before))\n",
    " \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes related to the model structure.\n",
    "Definitions of MLP, GeneratorFunc, DiscriminatorFunc, Discriminator.\n",
    "Generator will be written later in each cases as the solvers used will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipSwish(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.909 * torch.nn.functional.silu(x)\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_size, out_size, mlp_size, num_layers, tanh):\n",
    "        super().__init__()\n",
    "\n",
    "        model = [torch.nn.Linear(in_size, mlp_size),\n",
    "                 LipSwish()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            model.append(torch.nn.Linear(mlp_size, mlp_size))\n",
    "        model.append(torch.nn.Linear(mlp_size, out_size))\n",
    "        if tanh:\n",
    "            model.append(torch.nn.Tanh())\n",
    "        self._model = torch.nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "class GeneratorFunc(torch.nn.Module):\n",
    "    sde_type = 'stratonovich'\n",
    "    noise_type = 'general'\n",
    "\n",
    "    def __init__(self, noise_size, hidden_size, mlp_size, num_layers):\n",
    "        super().__init__()\n",
    "        self._noise_size = noise_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._drift = MLP(1 + hidden_size, hidden_size, mlp_size, num_layers, tanh=True)\n",
    "        self._diffusion = MLP(1 + hidden_size, hidden_size * noise_size, mlp_size, num_layers, tanh=True)\n",
    "\n",
    "    def f_and_g(self, t, x):\n",
    "        # t has shape ()\n",
    "        # x has shape (batch_size, hidden_size)\n",
    "        t = t.expand(x.size(0), 1)\n",
    "        tx = torch.cat([t, x], dim=1)\n",
    "        return self._drift(tx), self._diffusion(tx).view(x.size(0), self._hidden_size, self._noise_size)\n",
    "\n",
    "class DiscriminatorFunc(torch.nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, mlp_size, num_layers):\n",
    "        super().__init__()\n",
    "        self._data_size = data_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "        # tanh is important for model performance\n",
    "        self._module = MLP(1 + hidden_size, hidden_size * (1 + data_size), mlp_size, num_layers, tanh=True)\n",
    "\n",
    "    def forward(self, t, h):\n",
    "        # t has shape ()\n",
    "        # h has shape (batch_size, hidden_size)\n",
    "        t = t.expand(h.size(0), 1)\n",
    "        th = torch.cat([t, h], dim=1)\n",
    "        return self._module(th).view(h.size(0), self._hidden_size, 1 + self._data_size)\n",
    "\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, mlp_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self._initial = MLP(1 + data_size, hidden_size, mlp_size, num_layers, tanh=False)\n",
    "        self._func = DiscriminatorFunc(data_size, hidden_size, mlp_size, num_layers)\n",
    "        self._readout = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, ys_coeffs):\n",
    "        # ys_coeffs has shape (batch_size, t_size, 1 + data_size)\n",
    "        # The +1 corresponds to time. When solving CDEs, It turns out to be most natural to treat time as just another\n",
    "        # channel: in particular this makes handling irregular data quite easy, when the times may be different between\n",
    "        # different samples in the batch.\n",
    "\n",
    "        Y = torchcde.LinearInterpolation(ys_coeffs)\n",
    "        Y0 = Y.evaluate(Y.interval[0])\n",
    "        h0 = self._initial(Y0)\n",
    "        hs = torchcde.cdeint(Y, self._func, h0, Y.interval, method='reversible_heun', backend='torchsde', dt=1.0,\n",
    "                             adjoint_method='adjoint_reversible_heun',\n",
    "                             adjoint_params=(ys_coeffs,) + tuple(self._func.parameters()))\n",
    "        score = self._readout(hs[:, -1])\n",
    "        return score.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(ts, batch_size, dataloader, generator, discriminator):\n",
    "    with torch.no_grad():\n",
    "        total_samples = 0\n",
    "        total_loss = 0\n",
    "        for real_samples, in dataloader:\n",
    "            generated_samples = generator(ts, batch_size)\n",
    "            generated_score = discriminator(generated_samples)\n",
    "            real_score = discriminator(real_samples)\n",
    "            loss = generated_score - real_score\n",
    "            total_samples += batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_normalBP(torch.nn.Module):\n",
    "    def __init__(self, data_size, initial_noise_size, noise_size, hidden_size, mlp_size, num_layers):\n",
    "        super().__init__()\n",
    "        self._initial_noise_size = initial_noise_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "        self._initial = MLP(initial_noise_size, hidden_size, mlp_size, num_layers, tanh=False)\n",
    "        self._func = GeneratorFunc(noise_size, hidden_size, mlp_size, num_layers)\n",
    "        self._readout = torch.nn.Linear(hidden_size, data_size)\n",
    "\n",
    "    def forward(self, ts, batch_size):\n",
    "        # ts has shape (t_size,) and corresponds to the points we want to evaluate the SDE at.\n",
    "\n",
    "        ###################\n",
    "        # Actually solve the SDE.\n",
    "        ###################\n",
    "        init_noise = torch.randn(batch_size, self._initial_noise_size, device=ts.device)\n",
    "        x0 = self._initial(init_noise)\n",
    "\n",
    "        ###################\n",
    "        # We use the normal BP to get accurate gradients whilst using the adjoint method.\n",
    "        ###################\n",
    "        xs = torchsde.sdeint(self._func, x0, ts, method='midpoint', dt=1.0)\n",
    "        xs = xs.transpose(0, 1)\n",
    "        ys = self._readout(xs)\n",
    "\n",
    "        ###################\n",
    "        # Normalise the data to the form that the discriminator expects, in particular including time as a channel.\n",
    "        ###################\n",
    "        ts = ts.unsqueeze(0).unsqueeze(-1).expand(batch_size, ts.size(0), 1)\n",
    "        return torchcde.linear_interpolation_coeffs(torch.cat([ts, ys], dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:08<23:48:21,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:   0 Loss (unaveraged): 2.6824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 64/10000 [02:39<6:53:54,  2.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\UWMadisonResearch\\NeuralSDE_Tests\\ouSDE_accuracy.ipynb Cell 18\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     discriminator\u001b[39m.\u001b[39mload_state_dict(averaged_discriminator\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mstate_dict())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m generator, discriminator\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m generator_bp, discriminator_bp \u001b[39m=\u001b[39m trainingBP(ts, data_size, train_dataloader)\n",
      "\u001b[1;32mc:\\UWMadisonResearch\\NeuralSDE_Tests\\ouSDE_accuracy.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     mem_before \u001b[39m=\u001b[39m process_memory()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     mem_after \u001b[39m=\u001b[39m process_memory()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:consumed memory: \u001b[39m\u001b[39m{:,}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         mem_before, mem_after, mem_after \u001b[39m-\u001b[39m mem_before))\n",
      "\u001b[1;32mc:\\UWMadisonResearch\\NeuralSDE_Tests\\ouSDE_accuracy.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m real_score \u001b[39m=\u001b[39m discriminator(real_samples)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m loss \u001b[39m=\u001b[39m generated_score \u001b[39m-\u001b[39m real_score\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m generator\u001b[39m.\u001b[39mparameters():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/UWMadisonResearch/NeuralSDE_Tests/ouSDE_accuracy.ipynb#X23sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     param\u001b[39m.\u001b[39mgrad \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@profile\n",
    "\n",
    "def trainingBP(\n",
    "        # Architectural hyperparameters. These are quite small for illustrative purposes.\n",
    "        ts,             # How many timesteps to use for the SDE solver.\n",
    "        data_size,        # How many data points to use in the dataset.\n",
    "        train_dataloader,\n",
    "        initial_noise_size=5,  # How many noise dimensions to sample at the start of the SDE.\n",
    "        noise_size=3,          # How many dimensions the Brownian motion has.\n",
    "        hidden_size=16,        # How big the hidden size of the generator SDE and the discriminator CDE are.\n",
    "        mlp_size=16,           # How big the layers in the various MLPs are.\n",
    "        num_layers=1,          # How many hidden layers to have in the various MLPs.\n",
    "\n",
    "        # Training hyperparameters. Be prepared to tune these very carefully, as with any GAN.\n",
    "        generator_lr=2e-4,      # Learning rate often needs careful tuning to the problem.\n",
    "        discriminator_lr=1e-3,  # Learning rate often needs careful tuning to the problem.\n",
    "        batch_size=1024,        # Batch size.\n",
    "        steps=10000,            # How many steps to train both generator and discriminator for.\n",
    "        init_mult1=3,           # Changing the initial parameter size can help.\n",
    "        init_mult2=0.5,         #\n",
    "        weight_decay=0.01,      # Weight decay.\n",
    "        swa_step_start=5000,    # When to start using stochastic weight averaging.\n",
    "\n",
    "        # Evaluation and plotting hyperparameters\n",
    "        steps_per_print=100,                   # How often to print the loss.\n",
    "        device = 'cuda',                       # Which device to use.\n",
    "):\n",
    "    infinite_train_dataloader = (elem for it in iter(lambda: train_dataloader, None) for elem in it)\n",
    "    \n",
    "    # Models\n",
    "    generator = Generator_normalBP(data_size, initial_noise_size, noise_size, hidden_size, mlp_size, num_layers).to(device)\n",
    "    discriminator = Discriminator(data_size, hidden_size, mlp_size, num_layers).to(device)\n",
    "    # Weight averaging really helps with GAN training.\n",
    "    averaged_generator = swa_utils.AveragedModel(generator)\n",
    "    averaged_discriminator = swa_utils.AveragedModel(discriminator)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param in generator._initial.parameters():\n",
    "            param *= init_mult1\n",
    "        for param in generator._func.parameters():\n",
    "            param *= init_mult2\n",
    "\n",
    "    # Optimisers. Adadelta turns out to be a much better choice than SGD or Adam, interestingly.\n",
    "    generator_optimiser = torch.optim.Adadelta(generator.parameters(), lr=generator_lr, weight_decay=weight_decay)\n",
    "    discriminator_optimiser = torch.optim.Adadelta(discriminator.parameters(), lr=discriminator_lr,\n",
    "                                                   weight_decay=weight_decay)\n",
    "    \n",
    "    # Train both generator and discriminator.\n",
    "    t0 = time.time()\n",
    "\n",
    "    trange = tqdm.tqdm(range(steps))\n",
    "    for step in trange:\n",
    "        real_samples, = next(infinite_train_dataloader)\n",
    "\n",
    "        generated_samples = generator(ts, batch_size)\n",
    "        generated_score = discriminator(generated_samples)\n",
    "        real_score = discriminator(real_samples)\n",
    "        loss = generated_score - real_score\n",
    "        loss.backward()\n",
    "\n",
    "        for param in generator.parameters():\n",
    "            param.grad *= -1\n",
    "        generator_optimiser.step()\n",
    "        discriminator_optimiser.step()\n",
    "        generator_optimiser.zero_grad()\n",
    "        discriminator_optimiser.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for module in discriminator.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    lim = 1 / module.out_features\n",
    "                    module.weight.clamp_(-lim, lim)\n",
    "\n",
    "        # Stochastic weight averaging typically improves performance.\n",
    "        if step > swa_step_start:\n",
    "            averaged_generator.update_parameters(generator)\n",
    "            averaged_discriminator.update_parameters(discriminator)\n",
    "\n",
    "        if (step % steps_per_print) == 0 or step == steps - 1:\n",
    "            total_unaveraged_loss = evaluate_loss(ts, batch_size, train_dataloader, generator, discriminator)\n",
    "            if step > swa_step_start:\n",
    "                total_averaged_loss = evaluate_loss(ts, batch_size, train_dataloader, averaged_generator.module,\n",
    "                                                    averaged_discriminator.module)\n",
    "                trange.write(f\"Step: {step:3} Loss (unaveraged): {total_unaveraged_loss:.4f} \"\n",
    "                             f\"Loss (averaged): {total_averaged_loss:.4f}\")\n",
    "            else:\n",
    "                trange.write(f\"Step: {step:3} Loss (unaveraged): {total_unaveraged_loss:.4f}\")\n",
    "\n",
    "    print(\"Training time:\", time.time()-t0)\n",
    "    generator.load_state_dict(averaged_generator.module.state_dict())\n",
    "    discriminator.load_state_dict(averaged_discriminator.module.state_dict())\n",
    "    return generator, discriminator\n",
    "\n",
    "generator_bp, discriminator_bp = trainingBP(ts, data_size, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load trained model\n",
    "generator_bp = Generator_normalBP(1, 5, 3, 16, 16, 1)\n",
    "generator_bp.load_state_dict(torch.load('generator_bp.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorFunc(\n",
       "  (_drift): MLP(\n",
       "    (_model): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=16, bias=True)\n",
       "      (1): LipSwish()\n",
       "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (_diffusion): MLP(\n",
       "    (_model): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=16, bias=True)\n",
       "      (1): LipSwish()\n",
       "      (2): Linear(in_features=16, out_features=48, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_bp._func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0203, -0.1275,  0.0486,  0.0983, -0.1108, -0.0567,  0.0723, -0.0285,\n",
       "        -0.0550, -0.1302, -0.0040, -0.0209, -0.0710, -0.0257,  0.0682,  0.0004],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_bp._func._drift._model[2].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3242, -1.2287,  1.1754, -1.0612,  0.5608,  0.6426,  0.6576,  1.1208,\n",
       "        -0.4470, -1.2468, -0.6667,  0.0024, -0.1490,  1.2834, -0.8376,  1.1158],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_bp._initial._model[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralSDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
